\documentclass[landscape,twocolumn]{article}
\usepackage{biblatex, graphicx, hyperref, mathtools, pgfplots, pgfplotstable}
\addbibresource{references.bib}
\pgfplotsset{compat = 1.16}
\title{COMP5318 Assignment 1}
\author{Nicholas Grasevski (ngra5777, 500710654)}
\begin{document}
\maketitle
\begin{abstract}
In this report several image classifiers are benchmarked against a grayscale image dataset. With the help of some feature engineering, a linear Support Vector Machine is the highest performing classifier with 89.65\% accuracy on the test set.
\end{abstract}

\section{Introduction}
Object recognition is one of the biggest subfields of computer vision, with many practical applications such as computer aided diagnosis \cite{doi2007computer}, face detection \cite{hjelmaas2001face}, Optical Character Recognition \cite{mori1999optical} and so on.

The current state of the art in object recognition is Convolutional Neural Networks \cite{iandola2016squeezenet}. The convolutional neural network takes in the raw pixel data and applies several layers of "convolution" transformations, which aggregate nearby pixels into higher order features in a hierarchical fashion. Prior to the prevalence of GPU based neural network training, the previous state of the art was to apply some hand crafted transformations to the input image to extract higher order features \cite{rybski2010visual}, and then pass the encoded features through a classical machine learning algorithm such as a generalized linear model \cite{ebrahimzadeh2014efficient}.

The aim of this study is to compare and contrast several different machine learning algorithms when applied to an object detection problem. This should provide insight into their relative ability to capture the information in the data, with the help of some standard hand crafted feature transformations.

\section{Methods}
Describe the pre-processing techniques and classifier methods algorithms.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{../Dataset_image}
\caption{Grayscale 28x28 images of clothing.}
\label{fig:images}
\end{figure}

Several machine learning algorithms were benchmarked against a corpus of 28x28 grayscale images. Examples of the images can bee seen in figure \ref{fig:images}. There are 10 classes in total:
\begin{enumerate}
\item T-shirt/Top
\item Trouser
\item Pullover
\item Dress
\item Coat
\item Sandal
\item Shirt
\item Sneaker
\item Bag
\item Ankle Boot
\end{enumerate}

30,000 of these images are used for training and hyperparameter tuning using 10-fold cross-validation, and 2,000 are used as a hold-out set to evaluate the best classifier according to the hyperparameter tuning results.

The performance is evaluated primarily based on top-1 accuracy, as defined in equation \ref{eq:accuracy}:

\begin{equation}
\label{eq:accuracy}
Accuracy = \frac{\text{Number of correct classifications}}{\text{Total number of test examples used}}
\end{equation}

The training and inference time cost of each algorithm is also evaluated. The experiment was performed on a 16-inch 2019 MackBook Pro, with a 2.3GHz 8-Core Intel Core i9 and 32 GB 2667 MHz DDR4 RAM.

\subsection{Preprocessing}
A common preprocessing step is applied before doing the hyperparameter grid search across the various algorithms:
\begin{enumerate}
\item Normalize the raw image data and flatten into a vector.
\item Calculate the Histogram of Oriented Gradients vector on the raw image data.
\item Concatenate the two vectors to produce the final input vector for the machine learning algorithm.
\end{enumerate}

\subsubsection{Normalization}
\subsubsection{Histogram of Oriented Gradients}

\subsection{Algorithms}
Several different algorithms are applied in turn to the preprocessed data, and a grid of their hyperparameters are explored. A stratified 10-fold cross validation is performed on each hyperparameter combination and the highest scoring (algorithm, hyperparameter) combination is retrained on the entire training set and evaluated on the test set.

\subsubsection{Nearest Neighbor}
\subsubsection{Logistic Regression}
\subsubsection{Naive Bayes}
\subsubsection{Decision Tree}
\subsubsection{Bagging}
\subsubsection{Ada Boost}
\subsubsection{SVM}

\section{Experiments and results}
Comparing experiment results between different algorithms (using table or graph) and produce a meaningful discussion of results from the experiment and choice of classifier methods (analysis on potential reasons for good performance or bad performance, and for training and inference time consumption).

\section{Conclusion}
Meaningful conclusion based on results, future work, and improvement suggested.

\printbibliography\appendix
\section{Running the code}
\begin{itemize}
\item Dependencies: \texttt{pip3 install h5py pandas scikit-image scikit-learn}
\item Usage: run the steps in the notebook sequentially to get the tuning and evaluation results.
\end{itemize}
Detailed hyperparameter tuning results will be written as csv files to \texttt{Algorithm/Output} for each respective classifier, along with the final predictions as \texttt{Algorithm/Output/predicted\_labels.h5}.

\end{document}
